[![Waffle.io - Columns and their card count](https://badge.waffle.io/IS-BODAND/nhstatistics-core.svg?columns=all&style=flat-square)](https://waffle.io/IS-BODAND/nhstatistics-core)
[![GitHub license](https://img.shields.io/badge/license-Apache%20License%202.0-blue.svg?style=flat-square)](http://www.apache.org/licenses/LICENSE-2.0)
[![Release](https://jitpack.io/v/IS-BODAND/nhstatistics-core.svg?style=flat-square)](https://jitpack.io/#IS-BODAND/nhstatistics-core)
[![Build Status](https://travis-ci.org/IS-BODAND/nhstatistics-core.svg?branch=devel-1.3&style=flat-square)](https://travis-ci.org/IS-BODAND/nhstatistics-core)
[![codecov.io](https://codecov.io/gh/IS-BODAND/nhstatistics-core/branch/master/graphs/badge.svg?style=flat-square)](https://codecov.io/gh/IS-BODAND/nhstatistics-core)
[![CodeFactor](https://www.codefactor.io/repository/github/is-bodand/nhstatistics-core/badge?style=flat-square)](https://www.codefactor.io/repository/github/is-bodand/nhstatistics-core)

# NHentaiStatistics Core

The library you never though you would see. Core of the NHentaiStatistics program (soonâ„¢), allows the
download of data from the [nhentai](https://nhentai.net) site. Provides easy-to-use 'HentaiStream's to get and output doujin specified
by the famous ids. By using these streams one can easily get all data about one doujin without needing to work with
nhentai's non-existent library. (I really does *not* exist, one post on their blog says it exists, but that's just
a nasty lie. Most likely got lost in the https transmission.)  

## Why did I make this

Upon browsing the [r/Animemes](https://old.reddit.com/r/Animemes) subreddit, I saw a problem arise that could have easily been solved by knowing the
statistics of hentai tags on nhentai. I though, why isn't there a program that allows easy access to doujin on the site,
while providing all these *necessary* data. Sufficient to say, I wanted to so I sacrificed my
free time to the gods of lust and descended into my neat little basement of programming efficacy. When I emerged,
having overdosed on caffeine and suffering from great sleep-deprivation, I made myself be accompanied by this little
library to make it be the bringer of all data used in known processing.
